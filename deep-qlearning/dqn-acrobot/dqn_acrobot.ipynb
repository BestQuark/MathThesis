{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn-acrobot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxP1D4pVhty-"
      },
      "source": [
        "# Chapter 3 (Acrobot)\n",
        "### Author: Luis Mantilla (@BestQuark)\n",
        "This code was written for my undergraduate mathematics thesis on Deep Q-learning under the supervision of Dr. Mauricio Junca."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ_OsGZahsPY"
      },
      "source": [
        "#Reinforcement learning dependences\n",
        "!pip install keras-rl2 tensorflow gym > /dev/null 2>&1\n",
        "!apt-get install -y libglu1-mesa-dev libgl1-mesa-dev libosmesa6-dev xvfb ffmpeg curl patchelf libglfw3 libglfw3-dev cmake zlib1g zlib1g-dev swig > /dev/null 2>&1\n",
        "!pip install 'gym[all]' > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "! wget http://www.atarimania.com/roms/Roms.rar > /dev/null 2>&1\n",
        "! mkdir /content/ROM/ > /dev/null 2>&1\n",
        "! unrar e /content/Roms.rar /content/ROM/ > /dev/null 2>&1\n",
        "! python -m atari_py.import_roms /content/ROM/ > /dev/null 2>&1\n",
        "\n",
        "#Video rendering dependences\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLaTIRD3h3Pq"
      },
      "source": [
        "# Imports libraries \n",
        "from PIL import Image\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import seaborn as sns\n",
        "#sns.set_theme()\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Permute\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "from rl.agents import SARSAAgent, DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, MaxBoltzmannQPolicy, GreedyQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy\n",
        "from rl.memory import SequentialMemory, Memory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHSXR6keh7zV"
      },
      "source": [
        "#This block does video rendering in Colab. Retrieved from https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/. \n",
        "from gym.wrappers import Monitor\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyb7wPqMh-N9"
      },
      "source": [
        "#Defines the environment\n",
        "ENV_NAME = 'Acrobot-v1'\n",
        "env = gym.make(ENV_NAME)\n",
        "n_actions = env.action_space.n\n",
        "n_states = env.observation_space.shape[0]\n",
        "WINDOW_LENGTH = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiL4VNKAiAmn"
      },
      "source": [
        "#Contructs model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape = (WINDOW_LENGTH,n_states)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(n_actions, activation='linear'))\n",
        "\n",
        "#Summary of the constructed neural network\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1k8auDfiEDj"
      },
      "source": [
        "#Builds DQN agent\n",
        "memory = SequentialMemory(limit = 50_000, window_length = WINDOW_LENGTH)\n",
        "policy = BoltzmannQPolicy()\n",
        "dqn = DQNAgent(model=model, policy = policy, nb_actions = n_actions, memory=memory, \n",
        "               nb_steps_warmup = 200, target_model_update=1e-2)\n",
        "\n",
        "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70K4hS5BiF7B"
      },
      "source": [
        "#Trains DQN agent\n",
        "dqn_history = dqn.fit(env, nb_steps=100_000, visualize=False, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFl_Nl1eiILm"
      },
      "source": [
        "#Plots learning curve\n",
        "smooth = 20\n",
        "y = np.ones(smooth)\n",
        "x = np.asarray(dqn_history.history['episode_reward'])\n",
        "z = np.ones(len(x))\n",
        "smoothed_rewards = np.convolve(x,y,'same') / np.convolve(z,y,'same')\n",
        "\n",
        "plt.plot( dqn_history.history['nb_steps'] , dqn_history.history['episode_reward'],  alpha = 0.2, c='black', label = 'Raw')\n",
        "plt.plot(dqn_history.history['nb_steps'] , smoothed_rewards, c = 'b' , label = 'Smoothed')\n",
        "plt.title(f'DQNAgent in \"{ENV_NAME}\" environment')\n",
        "plt.legend()\n",
        "plt.xlabel('Steps');\n",
        "plt.ylabel('Reward');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMNF8rL8iKJZ"
      },
      "source": [
        "#Saves model's weight\n",
        "dqn.save_weights(f'/content/dqn_{ENV_NAME}_weights.h5f', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIKgwKyEiPzz"
      },
      "source": [
        "#Evaluates DQN model\n",
        "dqn.test(env, nb_episodes=5, visualize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92db36bBiRvL"
      },
      "source": [
        "# Makes a video of the agent\n",
        "env.close()\n",
        "env = wrap_env(gym.make(ENV_NAME)) \n",
        "for i in range(1):\n",
        "    score = 0\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = dqn.forward(state)\n",
        "        #action = env.action_space.sample()\n",
        "        state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "        env.render()\n",
        "        dqn.backward(reward, terminal = done)\n",
        "    print(f'Episode {i}, Score {score}')\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DVw7JAXiTiN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}